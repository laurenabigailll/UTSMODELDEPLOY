# -*- coding: utf-8 -*-
"""2602108426_ModelDeploy2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j5lfjqZ9_hfE_v-l4_pzMwPgHFRRmHG7
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
import pickle

class ChurnPrediction:
    def __init__(self, data_path):
        self.df = pd.read_csv(data_path)

    def preprocess_data(self):
        self.df.dropna(subset=['CreditScore'], inplace=True)
        self.df.drop(columns=['Unnamed: 0', 'id', 'CustomerId', 'Surname'], inplace=True)
        self.categorical_columns = ['Geography', 'Gender', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Tenure','churn']
        self.numeric_columns = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']
        self.input = self.df.drop('churn', axis=1)
        self.output = self.df['churn']
        pilih = ['Gender', 'Geography']
        self.OneHotEncoder = pd.get_dummies(self.input[pilih])
        self.input = pd.concat([self.input.drop(pilih, axis=1), self.OneHotEncoder], axis=1)
        use = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']
        self.Scaler = StandardScaler()
        self.input[use] = self.Scaler.fit_transform(self.input[use])

    def train_test_split(self):
        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.input, self.output, test_size=0.2, random_state=42)

    def random_forest_model(self):
        param_grid = {'n_estimators': [50, 100],
                      'criterion': ['gini', 'entropy'],
                      'max_depth': [None, 10]}
        rf_classifier = RandomForestClassifier(random_state=42)
        self.rf_classifier = GridSearchCV(rf_classifier,
                                           param_grid=param_grid,
                                           scoring='accuracy',
                                           cv=5)
        self.rf_classifier.fit(self.x_train, self.y_train)

    def xgboost_model(self):
        param_grid_xgb = {'n_estimators': [50, 100],
                          'max_depth': [3, 5],
                          'learning_rate': [0.1, 0.01],
                          'subsample': [0.5, 0.7]
                          }
        xgb_classifier = XGBClassifier(random_state=42)
        self.xgb_classifier = GridSearchCV(xgb_classifier,
                                            param_grid=param_grid_xgb,
                                            scoring='accuracy',
                                            cv=5)
        self.xgb_classifier.fit(self.x_train, self.y_train)

    def evaluate_models(self):
        rfclassifier = self.rf_classifier.best_estimator_
        y_predict_rf = rfclassifier.predict(self.x_test)
        print("Random Forest Classifier:")
        print(classification_report(self.y_test, y_predict_rf))

        xgbclassifier = self.xgb_classifier.best_estimator_
        y_predict_xgb = xgbclassifier.predict(self.x_test)
        print("XGBoost Classifier:")
        print(classification_report(self.y_test, y_predict_xgb))

    def save_model(self, model, filename):
        with open(filename, 'wb') as file:
            pickle.dump(model, file)

# Usage
if __name__ == "__main__":
    churn_pred = ChurnPrediction('/content/sample_data/data_D.csv')
    churn_pred.preprocess_data()
    churn_pred.train_test_split()
    churn_pred.random_forest_model()
    churn_pred.xgboost_model()
    churn_pred.evaluate_models()
    churn_pred.save_model(churn_pred.xgb_classifier.best_estimator_, 'xgb_classifier_model.pkl')

